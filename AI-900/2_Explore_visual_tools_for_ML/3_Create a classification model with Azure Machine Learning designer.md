## Evaluate performance
- Confusion matrix
  - **Accuracy**: The ratio of correct predictions (true positives + true negatives) to the total number of predictions.
  - **Precision**: The fraction of the cases classified as positive that are actually positive (the number of true positives divided by the number of true positives plus false positives).
  - **Recall**: The fraction of positive cases correctly identified (the number of true positives divided by the number of true positives plus false negatives).
  - **F1 Score**: An overall metric that essentially combines precision and recall.
- ROC curve and AUC metric
  - ROC: receiver operating characteristic
  - The larger the area under the ROC curve, of AUC metric, (which can be any value from 0 to 1), the better the model is performing.
